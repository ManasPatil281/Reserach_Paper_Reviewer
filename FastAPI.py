import os
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from langchain_groq import ChatGroq
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv
import tempfile
from fastapi import UploadFile, File
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import aiofiles
import traceback
import logging



# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

class TextRequest(BaseModel):
    text: str
    mode: str
    language: str
    options: str


HF_TOKEN = os.getenv("HF_TOKEN")
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
Groq_API=os.getenv("GROQ_api_key_Resume")

llm = ChatGroq(groq_api_key=Groq_API, model_name="llama-3.1-8b-instant")
llm_2 = ChatGroq(groq_api_key=Groq_API, model_name="gemma2-9b-it")
llm3 = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    api_key=os.getenv("GOOGLE_API_KEY", "AIzaSyDzJdYXogH--ueNwQF3z9o85Ln-iuOmG_s")
)

app = FastAPI()

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def process_pdf_file(file, process_func):
    try:
        # Save uploaded file to a temporary file
        async with aiofiles.tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf:
            await temp_pdf.write(await file.read())
            temp_pdf_path = temp_pdf.name

        # Process the PDF
        result = await process_func(temp_pdf_path)
        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing file: {e}")
    finally:
        # Remove the temporary file after processing
        if os.path.exists(temp_pdf_path):
            os.remove(temp_pdf_path)

@app.post("/AI_detect_pdf")
async def detect_ai_generated(file: UploadFile = File(...)):
    async def process_pdf(temp_pdf_path):
        try:
            # Define system prompt for AI detection
            system_prompt = (
                "Analyze the given content and determine whether it was generated by AI. "
                "Provide a detailed explanation and a percentage score indicating the likelihood "
                "of AI-generated content."
            )

            # Load the PDF using PyPDFLoader
            loader = PyPDFLoader(temp_pdf_path)
            docs = loader.load()

            # Text splitting for embeddings
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
            splits = text_splitter.split_documents(docs)

            # Create FAISS vectorstore for retrieval
            vectorstore = FAISS.from_documents(splits, embeddings)
            retriever = vectorstore.as_retriever()

            # Prepare QA prompt
            qa_prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "{context}\n{input}"),
            ])

            # Create chains
            question_answer_chain = create_stuff_documents_chain(llm3, qa_prompt)
            rag_chain = create_retrieval_chain(retriever, question_answer_chain)

            # Generate response
            response = rag_chain.invoke({"input": 'Detect if the content is AI-generated.'})
            return response["answer"]
        except Exception as e:
            logger.error(f"AI Detection Error: {str(e)}")
            logger.error(traceback.format_exc())
            raise

    try:
        # Validate file type
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="Only PDF files are allowed")

        # Save the uploaded file to a temporary location


        # Process the PDF for AI detection
        ai_detection_result = await process_pdf_file(file, process_pdf)
        return {"result": ai_detection_result}
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"AI Detect PDF Error: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Error detecting AI-generated content in PDF: {str(e)}")




@app.post("/AI_detect_text")
async def detect_text(request: TextRequest):
    """
    Detects whether the provided text input is AI-generated and provides a score.
    """
    system_prompt = (
        f"Detect whether the provided text '{request.text}' is AI-generated or not. "
        "Also provide a score of % of AI-generated content."
    )

    try:
        response = llm3.invoke(system_prompt)
        return {"result": response.content}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error detecting AI-generated content: {e}")

@app.post("/grammar-check")
async def grammar_check(request: TextRequest):
    """
    Analyzes and refines the content by performing a comprehensive grammar check.
    """
    system_prompt = (
        f"Analyze and refine the following content '{request.text}' by performing a comprehensive grammar check. "
        f"Identify and correct errors in spelling, punctuation, sentence structure, verb agreement, and word usage. "
        f"Provide suggestions to improve clarity, coherence, and tone. Maintain the original meaning while ensuring "
        f"the text adheres to the highest standards of grammar and style for '{request.language}' language. "
        f"Also, make sure to use the '{request.mode}' mode for the best results. "
        "results should be on par with results generated by Turnitin , Quillbot, and Grammarly. "
        "Also provide a score out of 100."
    )

    try:
        response = llm3.invoke(system_prompt)
        return {"result": response.content}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error performing grammar check: {e}")

@app.post("/paraphrase-text")
async def paraphrase_text(request: TextRequest):
    """
    Paraphrases the provided text input.
    """
    system_prompt = (
        f"Rephrase the following content '{request.text}' to enhance readability, clarity, or style, "
        f"based on the '{request.mode}' mode and using the '{request.language}' language. Preserve the original meaning while adapting "
        f"grammar, tone, and flow for the best result."
    )

    try:
        response = llm3.invoke(system_prompt)
        return {"result": response.content}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error paraphrasing text: {e}")

@app.post("/paraphrase-pdf")
async def paraphrase_pdf(file: UploadFile = File(...), mode: str = "default", language: str = "English"):
    async def process_pdf(temp_pdf_path):
        try:
            system_prompt = (
                f"Rephrase the following content to enhance readability, clarity, or style, "
                f"based on the '{mode}' mode and using the '{language}' language. Preserve the original meaning while adapting "
                f"grammar, tone, and flow for the best result."
            )

            # Load the PDF using PyPDFLoader
            loader = PyPDFLoader(temp_pdf_path)
            docs = loader.load()

            text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
            splits = text_splitter.split_documents(docs)

            vectorstore = FAISS.from_documents(splits, embeddings)
            retriever = vectorstore.as_retriever()

            qa_prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "{context}\n{input}"),
            ])

            question_answer_chain = create_stuff_documents_chain(llm3, qa_prompt)
            rag_chain = create_retrieval_chain(retriever, question_answer_chain)
            response = rag_chain.invoke({"input": 'paraphrase the text'})
            return response["answer"]
        except Exception as e:
            logger.error(f"PDF Paraphrasing Error: {str(e)}")
            logger.error(traceback.format_exc())
            raise

    try:
        # Validate file type
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="Only PDF files are allowed")

        paraphrased_text = await process_pdf_file(file, process_pdf)
        return {"result": paraphrased_text}
    except Exception as e:
        logger.error(f"Paraphrase PDF Error: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Error paraphrasing PDF content: {str(e)}")


@app.post("/detect-plagiarism")
async def detect_plagiarism(file: UploadFile = File(...)):
    """
    Detects plagiarism in an uploaded PDF file and provides a detailed report.
    """
    async def process_pdf(temp_pdf_path):
        # Load the PDF using PyPDFLoader
        loader = PyPDFLoader(temp_pdf_path)
        docs = loader.load()

        # Split text into chunks for embeddings
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
        splits = text_splitter.split_documents(docs)

        # Create FAISS vectorstore
        vectorstore = FAISS.from_documents(splits, embeddings)
        retriever = vectorstore.as_retriever()

        # Define the system prompt for plagiarism detection
        system_prompt = (
            "Detect plagiarism and create a detailed report similar to those generated by Quilbot Premium or Turnitin. "
            "The report should include the following sections:\n"
            "1. Overall plagiarism percentage.\n"
            "2. A detailed breakdown of sections with detected plagiarism, including the percentage of match for each section.\n"
            "3. List of external sources with URLs where the similar content was found.\n"
            "4. Specific sentences or phrases that are flagged as plagiarized.\n"
            "5. Suggestions for improving the text by rephrasing or paraphrasing flagged sections to enhance originality.\n"
            "6. Summary of the unique content and areas of the text that are free from plagiarism.\n"
            "7. Use code blocks for some important texts to highlight (not code), relevant tables, and figures.\n"
            "8. Provide a link to download the Word or text file of the generated report."
        )

        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{context}\n{input}"),
        ])

        # Assume the LLM object is preloaded
        question_answer_chain = create_stuff_documents_chain(llm3, qa_prompt)
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        # Input text for detection
        response = rag_chain.invoke({"input": "Detect plagiarism and create a detailed report."})

        # Extract detailed results
        return response["answer"]

    try:
        plagiarism_results = await process_pdf_file(file, process_pdf)
        return {"plagiarism_report": plagiarism_results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error detecting plagiarism: {e}")


@app.post("/summarize-text")
async def summarize_text_endpoint(request: TextRequest):
    try:
        system_prompt = (
            f"Summarize the following content: {request.text} to enhance readability, clarity, or style, "
            f"based on the '{request.mode}' mode and using the '{request.language}' language. "
            f"Preserve the original meaning while adapting."
        )

        # Log the incoming request details
        logger.info(f"Summarize Text Request: Mode={request.mode}, Language={request.language}")

        response = llm3.invoke(system_prompt)
        return {"summary": response.content}
    except Exception as e:
        # Log the full error traceback
        logger.error(f"Summarize Text Error: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Error summarizing text: {str(e)}")


@app.post("/summarize-file")
async def summarize_file(file: UploadFile = File(...), mode: str = "standard", language: str = "English"):
    async def process_pdf(temp_pdf_path):
        try:
            # Load the PDF using PyPDFLoader
            loader = PyPDFLoader(temp_pdf_path)
            docs = loader.load()

            # Log document loading
            logger.info(f"PDF Loaded: {temp_pdf_path}, Number of pages: {len(docs)}")

            # Split text for embeddings
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
            splits = text_splitter.split_documents(docs)

            # Create FAISS vectorstore for retrieval
            vectorstore = FAISS.from_documents(splits, embeddings)
            retriever = vectorstore.as_retriever()

            # Define the system prompt for summarization
            system_prompt = (
                f"Summarize the following content to enhance readability, clarity, or style, "
                f"based on the '{mode}' mode and using the '{language}' language. Preserve the original meaning while adapting."
            )
            qa_prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "{context}\n{input}"),
            ])

            # Assume the LLM object is preloaded
            question_answer_chain = create_stuff_documents_chain(llm3, qa_prompt)
            rag_chain = create_retrieval_chain(retriever, question_answer_chain)

            # Summarize the document
            response = rag_chain.invoke({"input": "Summarize the text."})
            return response["answer"]
        except Exception as e:
            logger.error(f"PDF Summarization Error: {str(e)}")
            logger.error(traceback.format_exc())
            raise

    try:
        # Validate file type
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="Only PDF files are allowed")

        paraphrased_text = await process_pdf_file(file, process_pdf)
        return {"summary": paraphrased_text}
    except Exception as e:
        logger.error(f"Summarize File Error: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Error summarizing file content: {str(e)}")


@app.post("/review-file")
async def review_file(file: UploadFile = File(...)):
    async def process_pdf(temp_pdf_path):
        # Load the PDF using PyPDFLoader
        loader = PyPDFLoader(temp_pdf_path)
        docs = loader.load()

        # Split text for embeddings
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
        splits = text_splitter.split_documents(docs)

        # Create FAISS vectorstore for retrieval
        vectorstore = FAISS.from_documents(splits, embeddings)
        retriever = vectorstore.as_retriever()

        # Define the system prompt for summarization
        system_prompt = (
            "Review the provided research paper and create a detailed report that evaluates its quality, originality, and adherence to academic standards. "
            "The report should include the following sections:\n"
            "1. **Abstract Analysis**: Summarize and critique the abstract, noting its clarity, relevance, and whether it adequately represents the research.\n"
            "2. **Introduction and Objectives**: Evaluate the introduction, including the research objectives and problem statement, for clarity, relevance, and alignment with the field of study.\n"
            "3. **Literature Review**: Assess the comprehensiveness and depth of the literature review, highlighting any missing references or gaps.\n"
            "4. **Methodology**: Critique the methodology, noting its appropriateness, reproducibility, and whether sufficient details are provided.\n"
            "5. **Results and Analysis**: Evaluate the results for clarity, correctness, and relevance, and assess whether the analysis supports the conclusions drawn.\n"
            "6. **Discussion and Interpretation**: Provide feedback on the discussion section, including its depth, relevance, and how well it connects the results to the research objectives.\n"
            "7. **Citations and References**: Check for proper citation formatting, adequacy of references, and any potential plagiarism.\n"
            "8. **Plagiarism Check**: Provide a plagiarism analysis, including:\n"
            "   - Overall plagiarism percentage.\n"
            "   - Sections with detected plagiarism and their percentage matches.\n"
            "   - List of external sources with URLs where similar content was found.\n"
            "   - Specific sentences or phrases flagged as plagiarized.\n"
            "9. **Suggestions for Improvement**: Provide actionable suggestions to improve the paper, including:\n"
            "   - Rephrasing or paraphrasing flagged sections.\n"
            "   - Enhancing clarity, depth, or structure.\n"
            "10. **Strengths and Originality**: Highlight the paper's unique contributions and strengths.\n"
            "11. **Formatting and Presentation**: Comment on the formatting, organization, grammar, and presentation quality of the paper.\n"
            "12. **Summary of Review**: Provide a concise summary of the review, including the overall impression and whether the paper meets academic standards.\n"
            "Use code blocks for highlighting important sections (no actual code), and provide relevant tables and figures as needed for clarity. "
            "Ensure the report is clear, professional, and includes a downloadable Word or text file of the generated review."
        )

        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{context}\n{input}"),
        ])

        # Assume the LLM object is preloaded
        question_answer_chain = create_stuff_documents_chain(llm3, qa_prompt)
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        # Summarize the document
        response = rag_chain.invoke({"input": "Review the provided research paper and create a detailed report that evaluates its quality, originality, and adherence to academic standards."})
        return response["answer"]

    try:
        paraphrased_text = await process_pdf_file(file, process_pdf)
        return {"summary": paraphrased_text}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error summarizing file content: {e}")


def main():
    import uvicorn
    uvicorn.run(app, host="127.0.0.5", port=8005)

if __name__ == "__main__":
    main()